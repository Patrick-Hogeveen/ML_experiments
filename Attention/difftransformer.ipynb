{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No fused RMSNorm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from multihead_diffatn import MultiheadDiffAttn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 50 # what is the maximum context length for predictions?\n",
    "max_iters = 50000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    def __init__(self, model_parallel_size, decoder_kv_attention_heads):\n",
    "        self.model_parallel_size = model_parallel_size\n",
    "        self.decoder_kv_attention_heads = decoder_kv_attention_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1148, -0.1523, -0.1151,  ...,  0.1382, -0.1887,  0.1293],\n",
      "         [ 0.0528, -0.1235, -0.1395,  ...,  0.0842, -0.1782,  0.0525],\n",
      "         [ 0.0442, -0.1384, -0.1104,  ...,  0.0028, -0.0887,  0.1332],\n",
      "         ...,\n",
      "         [ 0.0292, -0.0828, -0.1127,  ...,  0.0275, -0.0417,  0.2730],\n",
      "         [ 0.0387, -0.0851, -0.1072,  ...,  0.0216, -0.0468,  0.2674],\n",
      "         [ 0.0349, -0.0808, -0.1091,  ...,  0.0201, -0.0469,  0.2683]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "test_args = args(1, None)\n",
    "multihead = MultiheadDiffAttn(test_args, 200, 100,100)\n",
    "\n",
    "v = torch.rand(1, 400,200)\n",
    "\n",
    "\n",
    "print(multihead(v, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class positionFeedFoward(nn.Module):\n",
    "    def __init__(self, inp, hid, drop=0.0):\n",
    "        super(positionFeedFoward, self).__init__()\n",
    "        self.w1 = nn.Linear(inp,4*hid)\n",
    "        self.w2 = nn.Linear(4*hid,inp)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.w2(F.relu(self.w1(x)))\n",
    "        x = self.drop(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    '''Combinds MultiHeadedAttention and FeeForward, three layers'''\n",
    "    def __init__(self, nheads, embn, con_l, drop=0.0):\n",
    "        super(Decoder, self).__init__()\n",
    "        head_size = embn // nheads\n",
    "        #fix arguments\n",
    "\n",
    "        self.slf_attn = MultiheadDiffAttn(args(1, None), embn, head_size, nheads)\n",
    "        \n",
    "        self.ffn = positionFeedFoward(embn, embn, drop=drop)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(embn)\n",
    "        self.norm2 = nn.LayerNorm(embn)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.slf_attn(self.norm1(x), 0)\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7053,  1.0463,  0.2766,  ...,  0.6525, -0.3046,  0.6850],\n",
       "         [ 0.2670,  0.8096,  0.7059,  ...,  1.0027,  0.2972,  0.0287],\n",
       "         [ 0.2243,  1.0756,  0.6185,  ...,  0.1017, -0.5243,  0.6314],\n",
       "         ...,\n",
       "         [ 1.2037,  0.2817,  1.0194,  ...,  0.1746,  0.8499,  0.9789],\n",
       "         [ 0.3529,  0.8892,  1.2204,  ...,  0.7721,  0.0205,  0.1838],\n",
       "         [ 1.1805,  0.7231, -0.0279,  ...,  0.6330,  0.0888,  0.5993]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#heads, d_model, d_km d_v as per the paper\n",
    "enc = Decoder(8, 64, 512)\n",
    "#batches, dims, dimensionalityxn_heads\n",
    "\n",
    "v = torch.rand(1,512,64)\n",
    "\n",
    "\n",
    "enc(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pytoch version adapted from here https://pub.aimind.so/creating-sinusoidal-positional-embedding-from-scratch-in-pytorch-98c49e153d6\n",
    "\n",
    "class PosEncoding(nn.Module):\n",
    "    def __init__(self, hid, n_pos=200):\n",
    "        super(PosEncoding, self).__init__()\n",
    "\n",
    "        self.register_buffer('pos_table', self._get_sinusoid_encoding_table(n_pos, hid))\n",
    "\n",
    "    def _get_sinusoid_encoding_table(self, n_pos, hid):\n",
    "\n",
    "        if hid %2 != 0:\n",
    "            raise ValueError(\"Sinusoidal positional embedding cannot apply to odd token embedding dim={}\".format(hid))\n",
    "        \n",
    "        positions = torch.arange(0,n_pos).unsqueeze_(1)\n",
    "        embeds = torch.zeros(n_pos, hid)\n",
    "\n",
    "        denom = torch.pow(10000, 2 * torch.arange(0, hid//2)/2)\n",
    "        embeds[:, 0::2] = torch.sin(positions/denom)\n",
    "        embeds[:, 1::2] = torch.cos(positions/denom)\n",
    "        embeds = embeds.unsqueeze(0)\n",
    "\n",
    "        return embeds\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pos_table[:, :x.size(1)].clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class languageModel(nn.Module):\n",
    "    '''Decoder model'''\n",
    "    def __init__(\n",
    "            self, n_vocab, embn, n_layers, n_head, dropout=0.2 , con_l=200\n",
    "    ):\n",
    "        super(languageModel, self).__init__()\n",
    "        self.con_l = con_l\n",
    "        self.word_emb = nn.Embedding(n_vocab, embn)\n",
    "        self.pos_enc = nn.Embedding(con_l, embn)\n",
    "        self.stack = nn.Sequential(\n",
    "            *[Decoder( n_head, embn, con_l, drop=dropout) for _ in range(n_layers)]\n",
    "        )\n",
    "       \n",
    "        self.layer_norm = nn.LayerNorm(embn)\n",
    "        self.fc = nn.Linear(embn, n_vocab)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x, tar=None):\n",
    "        #batch, time\n",
    "        B, T = x.shape\n",
    "\n",
    "        tok = self.word_emb(x)\n",
    "        pos = self.pos_enc(torch.arange(T, device=device))\n",
    "        x = tok + pos\n",
    "        x = self.stack(x)\n",
    "        x = self.layer_norm(x)\n",
    "        logits = self.fc(x)\n",
    "\n",
    "        if tar is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            tar = tar.view(B*T)\n",
    "            loss = F.cross_entropy(logits, tar)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, x, max_length):\n",
    "        #x is a BxT array of in current context\n",
    "        for _ in range(max_length):\n",
    "            x_cond = x[:, -self.con_l:]\n",
    "            logits, loss = self(x_cond)\n",
    "            logits = logits[:,-1,:]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            x_next = torch.multinomial(probs, num_samples=1)\n",
    "            x = torch.cat((x, x_next), dim=1)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits = model(X)[0]\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = Y.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "step 0: train loss 4.3811, val loss 4.3752\n",
      "step 500: train loss 1.7066, val loss 1.8771\n",
      "step 1000: train loss 1.5281, val loss 1.7197\n",
      "step 1500: train loss 1.4254, val loss 1.6362\n",
      "step 2000: train loss 1.3697, val loss 1.6090\n",
      "step 2500: train loss 1.3299, val loss 1.5765\n",
      "step 3000: train loss 1.2921, val loss 1.5636\n",
      "step 3500: train loss 1.2673, val loss 1.5817\n",
      "step 4000: train loss 1.2330, val loss 1.5711\n",
      "step 4500: train loss 1.2097, val loss 1.5719\n",
      "step 5000: train loss 1.1834, val loss 1.5616\n",
      "step 5500: train loss 1.1602, val loss 1.5846\n",
      "step 6000: train loss 1.1391, val loss 1.5996\n",
      "step 6500: train loss 1.1096, val loss 1.5974\n",
      "step 7000: train loss 1.0849, val loss 1.6221\n",
      "step 7500: train loss 1.0607, val loss 1.6339\n",
      "step 8000: train loss 1.0350, val loss 1.6526\n",
      "step 8500: train loss 1.0078, val loss 1.6873\n",
      "step 9000: train loss 0.9834, val loss 1.6952\n",
      "step 9500: train loss 0.9522, val loss 1.7349\n",
      "step 10000: train loss 0.9255, val loss 1.7486\n",
      "step 10500: train loss 0.9014, val loss 1.7841\n",
      "step 11000: train loss 0.8769, val loss 1.8121\n",
      "step 11500: train loss 0.8456, val loss 1.8389\n",
      "step 12000: train loss 0.8192, val loss 1.8633\n",
      "step 12500: train loss 0.7966, val loss 1.8915\n",
      "step 13000: train loss 0.7669, val loss 1.9330\n",
      "step 13500: train loss 0.7467, val loss 1.9729\n",
      "step 14000: train loss 0.7229, val loss 2.0087\n",
      "step 14500: train loss 0.7031, val loss 2.0220\n",
      "step 15000: train loss 0.6816, val loss 2.0442\n",
      "step 15500: train loss 0.6619, val loss 2.0906\n",
      "step 16000: train loss 0.6387, val loss 2.1334\n",
      "step 16500: train loss 0.6213, val loss 2.1563\n",
      "step 17000: train loss 0.6050, val loss 2.1822\n",
      "step 17500: train loss 0.5878, val loss 2.2476\n",
      "step 18000: train loss 0.5715, val loss 2.2650\n",
      "step 18500: train loss 0.5590, val loss 2.3150\n",
      "step 19000: train loss 0.5458, val loss 2.3335\n",
      "step 19500: train loss 0.5311, val loss 2.3722\n",
      "step 20000: train loss 0.5227, val loss 2.4131\n",
      "step 20500: train loss 0.5045, val loss 2.4351\n",
      "step 21000: train loss 0.5014, val loss 2.4606\n",
      "step 21500: train loss 0.4919, val loss 2.5014\n",
      "step 22000: train loss 0.4809, val loss 2.5305\n",
      "step 22500: train loss 0.4698, val loss 2.5233\n",
      "step 23000: train loss 0.4678, val loss 2.5729\n",
      "step 23500: train loss 0.4584, val loss 2.5992\n",
      "step 24000: train loss 0.4523, val loss 2.6409\n",
      "step 24500: train loss 0.4473, val loss 2.6554\n",
      "step 25000: train loss 0.4426, val loss 2.6788\n",
      "step 25500: train loss 0.4345, val loss 2.6946\n",
      "step 26000: train loss 0.4308, val loss 2.7321\n",
      "step 26500: train loss 0.4256, val loss 2.7251\n",
      "step 27000: train loss 0.4222, val loss 2.7576\n",
      "step 27500: train loss 0.4183, val loss 2.7431\n",
      "step 28000: train loss 0.4113, val loss 2.7748\n",
      "step 28500: train loss 0.4099, val loss 2.8188\n",
      "step 29000: train loss 0.4089, val loss 2.8185\n",
      "step 29500: train loss 0.4001, val loss 2.8531\n",
      "step 30000: train loss 0.3990, val loss 2.8517\n",
      "step 30500: train loss 0.3964, val loss 2.8873\n",
      "step 31000: train loss 0.3922, val loss 2.9226\n",
      "step 31500: train loss 0.3886, val loss 2.9236\n",
      "step 32000: train loss 0.3844, val loss 2.9636\n",
      "step 32500: train loss 0.3842, val loss 2.9599\n",
      "step 33000: train loss 0.3827, val loss 3.0093\n",
      "step 33500: train loss 0.3785, val loss 3.0085\n",
      "step 34000: train loss 0.3784, val loss 2.9914\n",
      "step 34500: train loss 0.3734, val loss 3.0001\n",
      "step 35000: train loss 0.3717, val loss 3.0538\n",
      "step 35500: train loss 0.3708, val loss 3.0061\n",
      "step 36000: train loss 0.3673, val loss 3.0634\n",
      "step 36500: train loss 0.3662, val loss 3.0844\n",
      "step 37000: train loss 0.3654, val loss 3.0830\n",
      "step 37500: train loss 0.3606, val loss 3.0712\n",
      "step 38000: train loss 0.3600, val loss 3.1314\n",
      "step 38500: train loss 0.3618, val loss 3.0596\n",
      "step 39000: train loss 0.3572, val loss 3.1233\n",
      "step 39500: train loss 0.3557, val loss 3.1228\n",
      "step 40000: train loss 0.3552, val loss 3.1278\n",
      "step 40500: train loss 0.3537, val loss 3.1764\n",
      "step 41000: train loss 0.3522, val loss 3.1406\n",
      "step 41500: train loss 0.3512, val loss 3.1841\n",
      "step 42000: train loss 0.3485, val loss 3.1872\n",
      "step 42500: train loss 0.3477, val loss 3.2131\n",
      "step 43000: train loss 0.3472, val loss 3.2502\n",
      "step 43500: train loss 0.3454, val loss 3.1905\n",
      "step 44000: train loss 0.3453, val loss 3.2054\n",
      "step 44500: train loss 0.3431, val loss 3.2531\n",
      "step 45000: train loss 0.3404, val loss 3.2416\n",
      "step 45500: train loss 0.3412, val loss 3.3162\n",
      "step 46000: train loss 0.3391, val loss 3.2825\n",
      "step 46500: train loss 0.3384, val loss 3.2545\n",
      "step 47000: train loss 0.3401, val loss 3.2854\n",
      "step 47500: train loss 0.3366, val loss 3.3321\n",
      "step 48000: train loss 0.3366, val loss 3.3119\n",
      "step 48500: train loss 0.3336, val loss 3.3225\n",
      "step 49000: train loss 0.3344, val loss 3.3755\n",
      "step 49500: train loss 0.3335, val loss 3.3648\n",
      "step 49999: train loss 0.3318, val loss 3.3178\n",
      "\n",
      "Look forlords and warriors 'O from my son lies that did spit him but so,\n",
      "He was so ruled buttle pain in hearing a\n",
      "Taken the second mark oh thee off, and look on.\n",
      "Therefore thy kingdom lost? why, thou wilt quarrel and the law,\n",
      "Or best against the master merry, and my succeeding issue,\n",
      "Against the Duke of Exeter, what think'st thou we have told them of the place;\n",
      "This other will I slain.\n",
      "\n",
      "QUEEN MARGARET:\n",
      "The more we stay, the strange insus; and their uncle cozen'd\n",
      "Of enry a word soldier, many a Ch\n"
     ]
    }
   ],
   "source": [
    "model = languageModel(vocab_size,  384,6, 6, con_l=50\n",
    "    )\n",
    "model = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "#print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "print(next(model.parameters()).is_cuda)\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "\n",
    "best_loss = 100000\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss(model)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        if losses['train'] < best_loss:\n",
    "            torch.save(model, 'shake_spear.pth')\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    #B, T, C = logits.shape\n",
    "    #logits = logits.view(B*T, C)\n",
    "    #targets = yb.view(B*T)\n",
    "    #loss = F.cross_entropy(logits, targets)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(context, max_length=500)[0].tolist()))\n",
    "#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wherefore art thou dip'd? and wilt thou slay thyself?\n",
      "Thy other raw and you but stood up,\n",
      "Ere the sun and love I have learn'd these forty years,\n",
      "My native English, now I must forego:\n",
      "And now my tongue's use is to me no more\n",
      "Than death: therefore shall have no more.\n",
      "\n",
      "GLOUCESTER:\n",
      "That does again but ever!\n",
      "\n",
      "KING EDWARD IV:\n",
      "What said my soul is my sovereign, am put to\n",
      "unto him that hath our brother dubb'd them gentlewomen.\n",
      "Art more of a worthy couch, if I can be\n",
      "kep to try old holy friends: ome to Romeo,\n",
      "The rest and\n"
     ]
    }
   ],
   "source": [
    "sinp = \"wherefore art thou\"\n",
    "sinp = torch.tensor(encode(sinp)).unsqueeze(0).to(device)\n",
    "print(decode(model.generate(sinp, max_length=500)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
