{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import math\n",
    "from pickle import dump\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.vocab_size = 65 #32000 #Size of tokenizers vocabulary\n",
    "        self.d_model = 5120 #Hidden dimension of the model\n",
    "        self.n_layers = 2 #Number of transoformer blocks\n",
    "        self.n_heads = 8 #Number of attention heads\n",
    "        self.d_kv_comp = 128 #Latent dimesnion for compressed keys/values\n",
    "        self.d_rope = 16 #rotary embedding dimension applied to a subset of query/key heads\n",
    "        self.n_experts = 32 #Total number of routed experts\n",
    "        self.n_shared = 2 #number of always active experts\n",
    "        self.top_k = 2 #Number of experts activated per token\n",
    "        self.seq_len = 256 #Maximum length of sequence during training\n",
    "        self.batch_size = 1 #number of sequences to process in parallel\n",
    "        self.ffn_dim = 384 #hidden dimension of feed foward network\n",
    "        self.device_groups = 4 # For device-limited routing\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(config.d_model, config.ffn_dim)\n",
    "        self.w2 = nn.Linear(config.ffn_dim, config.d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w2(F.gelu(self.w1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, scale=40):\n",
    "        super().__init__()\n",
    "        assert dim % 2 == 0, \"Dimension must be even for rotary embeddings\"\n",
    "        self.dim = dim\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim//2, 2).float() / (dim//2)))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        self.scale = 40\n",
    "\n",
    "    def forward(self, seq_len):\n",
    "        t = torch.arange(seq_len, device=self.inv_freq.device).type_as(self.inv_freq) / self.scale\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        return torch.cat((freqs, freqs), dim=-1)\n",
    "\n",
    "def rotate_half(x):\n",
    "    x1, x2 = x.chunk(2, dim=-1)\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary(x, cos, sin):\n",
    "    \"\"\"\n",
    "    Apply rotary embeddings to the first half of x.\n",
    "    \"\"\"\n",
    "    # Split x into two parts: one for rotary embeddings and the other untouched\n",
    "    x_rot, x_base = x.split(cos.shape[-1], dim=-1)\n",
    "    # Apply rotary embeddings to the rotary part\n",
    "    x_rot = (x_rot * cos) + (rotate_half(x_rot) * sin)\n",
    "    # Concatenate the rotary-applied and base parts\n",
    "    return torch.cat([x_rot, x_base], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MemoryOptimizedMLA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.d_head = config.d_model // config.n_heads\n",
    "        self.split_dim = self.d_head - config.d_rope   \n",
    "\n",
    "        # Projections\n",
    "        self.W_dkv = nn.Linear(config.d_model, config.d_kv_comp)\n",
    "        self.W_dq = nn.Linear(config.d_model, config.d_kv_comp)\n",
    "\n",
    "        self.W_uk = nn.Linear(config.d_kv_comp, config.n_heads * self.split_dim)\n",
    "        self.W_uv = nn.Linear(config.d_kv_comp, config.n_heads * self.d_head)  \n",
    "        self.W_uq = nn.Linear(config.d_kv_comp, config.n_heads * self.split_dim)\n",
    "\n",
    "        self.W_qr = nn.Linear(config.d_kv_comp, config.n_heads * config.d_rope)\n",
    "        self.W_kr = nn.Linear(config.d_model, config.n_heads * config.d_rope)\n",
    "\n",
    "        self.rotary = RotaryEmbedding(config.d_rope)\n",
    "        self.output = nn.Linear(config.n_heads * self.d_head, config.d_model)\n",
    "\n",
    "    def forward(self, h, past_kv=None):\n",
    "        batch_size, seq_len, _ = h.shape\n",
    "\n",
    "        # KV Compression\n",
    "        c_kv = self.W_dkv(h)\n",
    "        k = self.W_uk(c_kv).view(batch_size, seq_len, config.n_heads, self.split_dim)\n",
    "        v = self.W_uv(c_kv).view(batch_size, seq_len, config.n_heads, self.d_head)\n",
    "\n",
    "        # Query Compression\n",
    "        c_q = self.W_dq(h)\n",
    "        q_base = self.W_uq(c_q).view(batch_size, seq_len, config.n_heads, self.split_dim)\n",
    "        q_rot = self.W_qr(c_q).view(batch_size, seq_len, config.n_heads, config.d_rope)\n",
    "\n",
    "        # Rotary embeddings with proper dimensions\n",
    "        rotary_emb = self.rotary(seq_len)\n",
    "        cos = torch.cos(rotary_emb).view(1, seq_len, 1, -1)  # [1, seq, 1, dim]\n",
    "        sin = torch.sin(rotary_emb).view(1, seq_len, 1, -1)\n",
    "\n",
    "        # Apply rotary embeddings\n",
    "        q_rot = apply_rotary(q_rot, cos, sin)\n",
    "        k_rot = apply_rotary(\n",
    "            self.W_kr(h).view(batch_size, seq_len, config.n_heads, config.d_rope),\n",
    "            cos, sin\n",
    "        )\n",
    "\n",
    "        q = torch.cat([q_base, q_rot], dim=-1)\n",
    "        k = torch.cat([k, k_rot], dim=-1)\n",
    "\n",
    "        # Attention computation\n",
    "        scores = torch.einsum(\"bqhd,bkhd->bhqk\", q, k) / math.sqrt(self.d_head)\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        out = torch.einsum(\"bhqk,bkhd->bqhd\", attn, v)\n",
    "\n",
    "        return self.output(out.contiguous().view(batch_size, seq_len, -1)), (c_kv, k_rot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSeekMoE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.shared_experts = nn.ModuleList([Expert() for _ in range(config.n_shared)])\n",
    "        self.routed_experts = nn.ModuleList([Expert() for _ in range(config.n_experts)])\n",
    "        self.gate = nn.Linear(config.d_model, config.n_experts)\n",
    "        self.aux_loss = 0.0\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Shared experts process all tokens\n",
    "        shared_out = sum(expert(x) for expert in self.shared_experts)\n",
    "\n",
    "        # Device-limited routing\n",
    "        routed_logits = self.gate(x)\n",
    "        probs = F.softmax(routed_logits, dim=-1)\n",
    "        topk_probs, topk_indices = probs.topk(config.top_k, dim=-1)\n",
    "\n",
    "        # Expert balance loss\n",
    "        expert_counts = torch.zeros(config.n_experts, device=x.device)\n",
    "        expert_counts.scatter_add_(0, topk_indices.view(-1),\n",
    "                                 torch.ones_like(topk_indices.view(-1), dtype=torch.float))\n",
    "        self.aux_loss += expert_counts.float().var() * 0.003  # Î±1 from paper\n",
    "\n",
    "        # Sparse computation\n",
    "        routed_out = torch.zeros_like(x)\n",
    "        for k in range(config.top_k):\n",
    "            expert_mask = topk_indices[..., k]\n",
    "            expert_contrib = torch.zeros_like(x)\n",
    "\n",
    "            for expert_idx in range(config.n_experts):\n",
    "                mask = (expert_mask == expert_idx)\n",
    "                if mask.any():\n",
    "                    expert_out = self.routed_experts[expert_idx](x[mask])\n",
    "                    expert_contrib[mask] = expert_out * topk_probs[..., k][mask].unsqueeze(-1)\n",
    "\n",
    "            routed_out += expert_contrib\n",
    "\n",
    "        return shared_out + routed_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(config.d_model)\n",
    "        self.attn = MemoryOptimizedMLA()\n",
    "        self.norm2 = nn.LayerNorm(config.d_model)\n",
    "        self.moe = DeepSeekMoE()\n",
    "\n",
    "    def forward(self, x, past_kv=None):\n",
    "        # Attention with KV cache\n",
    "        attn_out, new_kv = checkpoint(self.attn, self.norm1(x), past_kv)\n",
    "        x = x + attn_out\n",
    "\n",
    "        # MoE with checkpointing\n",
    "        moe_out = checkpoint(self.moe, self.norm2(x))\n",
    "        x = x + moe_out\n",
    "\n",
    "        return x, new_kv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSeekV2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock() for _ in range(config.n_layers)])\n",
    "        self.norm = nn.LayerNorm(config.d_model)\n",
    "        self.lm_head = nn.Linear(config.d_model, config.vocab_size)\n",
    "\n",
    "        # Better initialization with residual scaling\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_normal_(p, gain=0.1/math.sqrt(config.n_layers))\n",
    "        # Add residual scaling\n",
    "        for block in self.blocks:\n",
    "            block.attn.output.weight.data.mul_(0.1)\n",
    "            block.moe.shared_experts[0].w2.weight.data.mul_(0.1)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embed(input_ids)\n",
    "        total_aux_loss = 0.0\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x, _ = block(x)\n",
    "            total_aux_loss += block.moe.aux_loss\n",
    "\n",
    "        return self.lm_head(self.norm(x)), total_aux_loss\n",
    "    \n",
    "    def generate(self, x, max_length):\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            x_cond = x[:, -config.seq_len:]\n",
    "            logits, loss = self(x_cond)\n",
    "            logits = logits[:,-1,:]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            x_next = torch.multinomial(probs, num_samples=1)\n",
    "            x = torch.cat((x,x_next), dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - config.seq_len, (config.batch_size,))\n",
    "    x = torch.stack([data[i:i+config.seq_len+1] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+config.seq_len+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "#@torch.no_grad()\n",
    "#def estimate_loss(model):\n",
    "#    out = {}\n",
    "#    model.eval()\n",
    "#    for split in ['train', 'val']:\n",
    "#        losses = torch.zeros(eval_iters)\n",
    "#        for k in range(eval_iters):\n",
    "#            X, Y = get_batch(split)\n",
    "#            logits = model(X)[0]\n",
    "#            B, T, C = logits.shape\n",
    "#            logits = logits.view(B*T, C)\n",
    "#            targets = Y.view(B*T)\n",
    "#            loss = F.cross_entropy(logits, targets)\n",
    "#            losses[k] = loss.item()\n",
    "#        out[split] = losses.mean()\n",
    "#    model.train()\n",
    "#    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    model = model\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.1)\n",
    "    optimizer_dict = {p: torch.optim.Adam([p], foreach=False, eps=3e-4) for p in model.parameters()}\n",
    "\n",
    "    #def optimizer_hook(parameter) -> None:\n",
    "    #    optimizer_dict[parameter].step()\n",
    "    #    optimizer_dict[parameter].zero_grad()\n",
    "#\n",
    "    #for p in model.parameters():\n",
    "    #    p.register_post_accumulate_grad_hook(optimizer_hook)\n",
    "    \n",
    "    lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=3e-4,\n",
    "        total_steps=40,\n",
    "        pct_start=0.1,\n",
    "    )\n",
    "\n",
    "    for epoch in range(40):\n",
    "        \n",
    "        xb, yb = get_batch('train')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.amp.autocast(device_type=\"cuda\"):\n",
    "            logits, aux_loss = model(xb[:,:-1])\n",
    "            loss = F.cross_entropy(logits.view(-1, config.vocab_size),\n",
    "                                   xb[:,1:].contiguous().view(-1))\n",
    "            loss += 0.0001 * aux_loss\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        print(f\"Epoch {epoch} Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dict is not an Optimizer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39m_record_memory_history(enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#print(count_parameters(model))\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m s \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39m_snapshot()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf16.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[0;32mIn[9], line 14\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      5\u001b[0m     optimizer_dict \u001b[38;5;241m=\u001b[39m {p: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam([p], foreach\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3e-4\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters()}\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m#def optimizer_hook(parameter) -> None:\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m#    optimizer_dict[parameter].step()\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m#    optimizer_dict[parameter].zero_grad()\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m#for p in model.parameters():\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m#    p.register_post_accumulate_grad_hook(optimizer_hook)\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     lr_scheduler \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOneCycleLR\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpct_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m40\u001b[39m):\n\u001b[1;32m     23\u001b[0m         xb, yb \u001b[38;5;241m=\u001b[39m get_batch(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:1975\u001b[0m, in \u001b[0;36mOneCycleLR.__init__\u001b[0;34m(self, optimizer, max_lr, total_steps, epochs, steps_per_epoch, pct_start, anneal_strategy, cycle_momentum, base_momentum, max_momentum, div_factor, final_div_factor, three_phase, last_epoch, verbose)\u001b[0m\n\u001b[1;32m   1955\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m   1956\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1957\u001b[0m     optimizer: Optimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1972\u001b[0m ):  \u001b[38;5;66;03m# noqa: D107\u001b[39;00m\n\u001b[1;32m   1973\u001b[0m     \u001b[38;5;66;03m# Validate optimizer\u001b[39;00m\n\u001b[1;32m   1974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(optimizer, Optimizer):\n\u001b[0;32m-> 1975\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(optimizer)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not an Optimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1976\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m optimizer\n\u001b[1;32m   1978\u001b[0m     \u001b[38;5;66;03m# Validate total_steps\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: dict is not an Optimizer"
     ]
    }
   ],
   "source": [
    "model = DeepSeekV2()\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "torch.cuda.memory._record_memory_history(enabled='all')\n",
    "\n",
    "#print(count_parameters(model))\n",
    "train(model)\n",
    "\n",
    "s = torch.cuda.memory._snapshot()\n",
    "with open(f\"f16.pickle\", \"wb\") as f:\n",
    "    dump(s, f)\n",
    "\n",
    "# tell CUDA to stop recording memory allocations now\n",
    "torch.cuda.memory._record_memory_history(enabled=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'mini_deepseek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23162/1894385882.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('mini_deepseek'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DeepSeekV2().to(device)\n",
    "model.load_state_dict(torch.load('mini_deepseek'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patrick/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 518])\n",
      "wherefore art thouy\n",
      "lnt ik \n",
      "k\n",
      "dcowd,eh i ln,e\n",
      "tK\n",
      "\n",
      ":ireokMoaeAu eyaSvhrna mMmdalaha\n",
      "hy:ar hthheNuhqhrthvytbar dtlahoaoenaw yr myhd,fnsdn uy oeo h\n",
      "nowns\n",
      "mtof idih iot m l ndil\n",
      "n,\n",
      "eg ireeesenhmin latiHmlidrov EsS nne\n",
      "vk  nmhhoilera meselhlindyme l.tlhuhr on  lhy: tu\n",
      "rMnisslhlwtye nlnhnUu nyr\n",
      "opeeelavlE\n",
      "nk:\n",
      "hylhodlmotSkkleoal nso  hdhCeiiyyl eyetm dosrivE\n",
      " ee\n",
      "hieehs  sLh\n",
      "\n",
      "Aehhore\n",
      "hehka\n",
      "mnunruft A sonr \n",
      "ryh!nm:hmt rey aleooata,fah \n",
      "rred d So .hrynyNlu\n",
      "!ar,\n",
      "yddiaa\n",
      "aadaal yhihEohesndhuih nNuk earanny  ry tsmNhan yEdvNhy\n"
     ]
    }
   ],
   "source": [
    "inp = \"wherefore art thou\"\n",
    "inp = torch.tensor(encode(inp)).unsqueeze(0).to(device)\n",
    "print(inp[0].shape)\n",
    "output = model.generate(inp, max_length=500)\n",
    "print(output.shape)\n",
    "print(decode(output[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
