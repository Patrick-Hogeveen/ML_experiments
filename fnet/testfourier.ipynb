{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fourier\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 50 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.random(1024)\n",
    "np.allclose(fourier.slow_dft(x) , np.fft.fft(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.2 ms ± 2.21 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "16.3 μs ± 507 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit fourier.slow_dft(x)\n",
    "%timeit np.fft.fft(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.random(1024)\n",
    "np.allclose(fourier.fft(x) , np.fft.fft(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.1 ms ± 2.89 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "1.86 ms ± 12.9 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "15.6 μs ± 230 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit fourier.slow_dft(x)\n",
    "%timeit fourier.fft(x)\n",
    "%timeit np.fft.fft(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 10, 10])\n",
      "torch.Size([3, 10, 10, 2])\n",
      "torch.Size([3, 10, 10, 2])\n",
      "torch.Size([3, 10, 10])\n",
      "tensor([[[[ 23.4951,   0.0000],\n",
      "          [ 77.8579,   0.0000],\n",
      "          [ 52.8417,   0.0000],\n",
      "          [209.0028,   0.0000],\n",
      "          [175.5370,   0.0000],\n",
      "          [  8.5961,   0.0000],\n",
      "          [ 60.1165,   0.0000],\n",
      "          [126.7314,   0.0000],\n",
      "          [186.9294,   0.0000],\n",
      "          [148.6850,   0.0000]],\n",
      "\n",
      "         [[ 21.0111,   0.0000],\n",
      "          [227.6316,   0.0000],\n",
      "          [ 16.5947,   0.0000],\n",
      "          [182.8949,   0.0000],\n",
      "          [226.3931,   0.0000],\n",
      "          [203.6695,   0.0000],\n",
      "          [253.8217,   0.0000],\n",
      "          [165.8158,   0.0000],\n",
      "          [162.2757,   0.0000],\n",
      "          [254.9681,   0.0000]],\n",
      "\n",
      "         [[287.2652,   0.0000],\n",
      "          [163.0083,   0.0000],\n",
      "          [  6.5721,   0.0000],\n",
      "          [ 29.3755,   0.0000],\n",
      "          [156.7918,   0.0000],\n",
      "          [178.3644,   0.0000],\n",
      "          [222.7991,   0.0000],\n",
      "          [108.3678,   0.0000],\n",
      "          [139.2534,   0.0000],\n",
      "          [166.1588,   0.0000]],\n",
      "\n",
      "         [[ 12.7831,   0.0000],\n",
      "          [139.9509,   0.0000],\n",
      "          [125.5512,   0.0000],\n",
      "          [ 52.3265,   0.0000],\n",
      "          [208.9944,   0.0000],\n",
      "          [ 79.4095,   0.0000],\n",
      "          [ 85.8270,   0.0000],\n",
      "          [265.0568,   0.0000],\n",
      "          [ 93.7975,   0.0000],\n",
      "          [134.6161,   0.0000]],\n",
      "\n",
      "         [[127.2725,   0.0000],\n",
      "          [121.4351,   0.0000],\n",
      "          [204.1686,   0.0000],\n",
      "          [ 65.1949,   0.0000],\n",
      "          [  7.7407,   0.0000],\n",
      "          [ 82.7763,   0.0000],\n",
      "          [271.8460,   0.0000],\n",
      "          [160.8336,   0.0000],\n",
      "          [ 25.1880,   0.0000],\n",
      "          [ 30.0603,   0.0000]],\n",
      "\n",
      "         [[214.0746,   0.0000],\n",
      "          [181.2290,   0.0000],\n",
      "          [ 26.0700,   0.0000],\n",
      "          [201.6676,   0.0000],\n",
      "          [ 34.5139,   0.0000],\n",
      "          [239.8222,   0.0000],\n",
      "          [298.1939,   0.0000],\n",
      "          [289.8924,   0.0000],\n",
      "          [229.2516,   0.0000],\n",
      "          [ 17.5943,   0.0000]],\n",
      "\n",
      "         [[181.1922,   0.0000],\n",
      "          [230.5292,   0.0000],\n",
      "          [272.9868,   0.0000],\n",
      "          [205.6035,   0.0000],\n",
      "          [172.8082,   0.0000],\n",
      "          [ 22.2993,   0.0000],\n",
      "          [231.2370,   0.0000],\n",
      "          [228.8740,   0.0000],\n",
      "          [ 23.6858,   0.0000],\n",
      "          [212.3212,   0.0000]],\n",
      "\n",
      "         [[ 38.0806,   0.0000],\n",
      "          [ 74.4213,   0.0000],\n",
      "          [171.0240,   0.0000],\n",
      "          [166.0731,   0.0000],\n",
      "          [156.7827,   0.0000],\n",
      "          [202.5620,   0.0000],\n",
      "          [144.8888,   0.0000],\n",
      "          [110.3088,   0.0000],\n",
      "          [ 88.3069,   0.0000],\n",
      "          [ 63.9522,   0.0000]],\n",
      "\n",
      "         [[265.9257,   0.0000],\n",
      "          [251.3316,   0.0000],\n",
      "          [139.6694,   0.0000],\n",
      "          [284.0551,   0.0000],\n",
      "          [236.5329,   0.0000],\n",
      "          [220.7447,   0.0000],\n",
      "          [  4.4483,   0.0000],\n",
      "          [289.1667,   0.0000],\n",
      "          [268.8907,   0.0000],\n",
      "          [107.3503,   0.0000]],\n",
      "\n",
      "         [[212.5686,   0.0000],\n",
      "          [121.9624,   0.0000],\n",
      "          [153.0602,   0.0000],\n",
      "          [212.3776,   0.0000],\n",
      "          [180.1579,   0.0000],\n",
      "          [ 73.0103,   0.0000],\n",
      "          [263.3501,   0.0000],\n",
      "          [230.0607,   0.0000],\n",
      "          [ 17.2166,   0.0000],\n",
      "          [174.2810,   0.0000]]],\n",
      "\n",
      "\n",
      "        [[[  4.5585,   0.0000],\n",
      "          [142.3527,   0.0000],\n",
      "          [ 47.3131,   0.0000],\n",
      "          [  9.8325,   0.0000],\n",
      "          [158.7455,   0.0000],\n",
      "          [197.8389,   0.0000],\n",
      "          [259.1360,   0.0000],\n",
      "          [ 17.0655,   0.0000],\n",
      "          [ 26.5702,   0.0000],\n",
      "          [215.2896,   0.0000]],\n",
      "\n",
      "         [[189.5171,   0.0000],\n",
      "          [ 53.7480,   0.0000],\n",
      "          [126.7499,   0.0000],\n",
      "          [265.1505,   0.0000],\n",
      "          [ 70.6316,   0.0000],\n",
      "          [131.9029,   0.0000],\n",
      "          [188.2923,   0.0000],\n",
      "          [187.5067,   0.0000],\n",
      "          [105.3509,   0.0000],\n",
      "          [ 44.5508,   0.0000]],\n",
      "\n",
      "         [[206.9349,   0.0000],\n",
      "          [179.3750,   0.0000],\n",
      "          [  7.8293,   0.0000],\n",
      "          [242.3924,   0.0000],\n",
      "          [185.6352,   0.0000],\n",
      "          [117.3763,   0.0000],\n",
      "          [289.1683,   0.0000],\n",
      "          [227.9430,   0.0000],\n",
      "          [213.1407,   0.0000],\n",
      "          [250.3390,   0.0000]],\n",
      "\n",
      "         [[285.8528,   0.0000],\n",
      "          [293.2285,   0.0000],\n",
      "          [112.3298,   0.0000],\n",
      "          [  0.9844,   0.0000],\n",
      "          [ 77.2659,   0.0000],\n",
      "          [294.5697,   0.0000],\n",
      "          [295.8330,   0.0000],\n",
      "          [234.0199,   0.0000],\n",
      "          [ 78.6113,   0.0000],\n",
      "          [234.8698,   0.0000]],\n",
      "\n",
      "         [[256.0861,   0.0000],\n",
      "          [221.3190,   0.0000],\n",
      "          [245.4673,   0.0000],\n",
      "          [297.3487,   0.0000],\n",
      "          [195.9584,   0.0000],\n",
      "          [141.2700,   0.0000],\n",
      "          [236.5265,   0.0000],\n",
      "          [166.9491,   0.0000],\n",
      "          [200.9186,   0.0000],\n",
      "          [ 88.2679,   0.0000]],\n",
      "\n",
      "         [[190.9387,   0.0000],\n",
      "          [247.4212,   0.0000],\n",
      "          [193.0511,   0.0000],\n",
      "          [138.0083,   0.0000],\n",
      "          [251.4148,   0.0000],\n",
      "          [107.5751,   0.0000],\n",
      "          [223.8442,   0.0000],\n",
      "          [217.9954,   0.0000],\n",
      "          [ 57.5911,   0.0000],\n",
      "          [189.2086,   0.0000]],\n",
      "\n",
      "         [[207.3588,   0.0000],\n",
      "          [ 85.2420,   0.0000],\n",
      "          [289.8805,   0.0000],\n",
      "          [186.1927,   0.0000],\n",
      "          [ 38.9951,   0.0000],\n",
      "          [253.2901,   0.0000],\n",
      "          [153.6100,   0.0000],\n",
      "          [230.0046,   0.0000],\n",
      "          [159.2540,   0.0000],\n",
      "          [  5.2155,   0.0000]],\n",
      "\n",
      "         [[ 29.4798,   0.0000],\n",
      "          [185.6910,   0.0000],\n",
      "          [225.0405,   0.0000],\n",
      "          [  3.6669,   0.0000],\n",
      "          [223.5167,   0.0000],\n",
      "          [217.2409,   0.0000],\n",
      "          [ 73.9507,   0.0000],\n",
      "          [218.1013,   0.0000],\n",
      "          [104.4556,   0.0000],\n",
      "          [ 50.9422,   0.0000]],\n",
      "\n",
      "         [[264.8647,   0.0000],\n",
      "          [ 86.3658,   0.0000],\n",
      "          [114.0745,   0.0000],\n",
      "          [249.6844,   0.0000],\n",
      "          [ 51.8479,   0.0000],\n",
      "          [207.4627,   0.0000],\n",
      "          [119.0959,   0.0000],\n",
      "          [276.2213,   0.0000],\n",
      "          [279.8814,   0.0000],\n",
      "          [ 26.1208,   0.0000]],\n",
      "\n",
      "         [[ 24.7802,   0.0000],\n",
      "          [198.8270,   0.0000],\n",
      "          [184.4317,   0.0000],\n",
      "          [108.1788,   0.0000],\n",
      "          [ 49.4818,   0.0000],\n",
      "          [ 88.0829,   0.0000],\n",
      "          [180.7710,   0.0000],\n",
      "          [ 67.0135,   0.0000],\n",
      "          [ 94.3279,   0.0000],\n",
      "          [ 95.4379,   0.0000]]],\n",
      "\n",
      "\n",
      "        [[[  2.3332,   0.0000],\n",
      "          [ 88.5597,   0.0000],\n",
      "          [ 62.8136,   0.0000],\n",
      "          [ 46.0074,   0.0000],\n",
      "          [ 99.4862,   0.0000],\n",
      "          [154.6520,   0.0000],\n",
      "          [155.0573,   0.0000],\n",
      "          [257.2540,   0.0000],\n",
      "          [ 66.2274,   0.0000],\n",
      "          [290.9909,   0.0000]],\n",
      "\n",
      "         [[136.2593,   0.0000],\n",
      "          [ 46.4050,   0.0000],\n",
      "          [ 30.9500,   0.0000],\n",
      "          [143.6417,   0.0000],\n",
      "          [222.4816,   0.0000],\n",
      "          [200.4478,   0.0000],\n",
      "          [267.9243,   0.0000],\n",
      "          [228.7699,   0.0000],\n",
      "          [  5.4707,   0.0000],\n",
      "          [ 35.8657,   0.0000]],\n",
      "\n",
      "         [[121.3029,   0.0000],\n",
      "          [ 55.7284,   0.0000],\n",
      "          [264.5975,   0.0000],\n",
      "          [231.1380,   0.0000],\n",
      "          [ 64.2339,   0.0000],\n",
      "          [114.7014,   0.0000],\n",
      "          [ 91.5010,   0.0000],\n",
      "          [239.1461,   0.0000],\n",
      "          [240.8268,   0.0000],\n",
      "          [279.8974,   0.0000]],\n",
      "\n",
      "         [[208.6180,   0.0000],\n",
      "          [170.6921,   0.0000],\n",
      "          [ 56.5331,   0.0000],\n",
      "          [212.9647,   0.0000],\n",
      "          [121.6083,   0.0000],\n",
      "          [216.6049,   0.0000],\n",
      "          [205.2364,   0.0000],\n",
      "          [294.6563,   0.0000],\n",
      "          [172.0001,   0.0000],\n",
      "          [295.5534,   0.0000]],\n",
      "\n",
      "         [[ 99.1991,   0.0000],\n",
      "          [ 73.4570,   0.0000],\n",
      "          [149.7598,   0.0000],\n",
      "          [188.4409,   0.0000],\n",
      "          [261.3910,   0.0000],\n",
      "          [ 36.9725,   0.0000],\n",
      "          [297.0513,   0.0000],\n",
      "          [281.0991,   0.0000],\n",
      "          [  4.1690,   0.0000],\n",
      "          [263.1960,   0.0000]],\n",
      "\n",
      "         [[230.5708,   0.0000],\n",
      "          [273.6947,   0.0000],\n",
      "          [ 92.9854,   0.0000],\n",
      "          [203.6551,   0.0000],\n",
      "          [219.8732,   0.0000],\n",
      "          [182.3720,   0.0000],\n",
      "          [117.7622,   0.0000],\n",
      "          [  8.3582,   0.0000],\n",
      "          [280.4752,   0.0000],\n",
      "          [177.1408,   0.0000]],\n",
      "\n",
      "         [[ 10.6868,   0.0000],\n",
      "          [ 69.4297,   0.0000],\n",
      "          [129.5409,   0.0000],\n",
      "          [ 20.2567,   0.0000],\n",
      "          [256.0238,   0.0000],\n",
      "          [113.6847,   0.0000],\n",
      "          [286.5862,   0.0000],\n",
      "          [126.8190,   0.0000],\n",
      "          [ 82.9184,   0.0000],\n",
      "          [280.1331,   0.0000]],\n",
      "\n",
      "         [[226.2163,   0.0000],\n",
      "          [102.9874,   0.0000],\n",
      "          [ 32.4145,   0.0000],\n",
      "          [  1.9337,   0.0000],\n",
      "          [150.6180,   0.0000],\n",
      "          [250.8619,   0.0000],\n",
      "          [ 74.2006,   0.0000],\n",
      "          [245.6798,   0.0000],\n",
      "          [184.4855,   0.0000],\n",
      "          [ 84.5555,   0.0000]],\n",
      "\n",
      "         [[ 83.8827,   0.0000],\n",
      "          [243.6698,   0.0000],\n",
      "          [263.7338,   0.0000],\n",
      "          [243.1683,   0.0000],\n",
      "          [164.4037,   0.0000],\n",
      "          [ 28.9788,   0.0000],\n",
      "          [236.8713,   0.0000],\n",
      "          [165.9432,   0.0000],\n",
      "          [ 15.4653,   0.0000],\n",
      "          [ 73.6467,   0.0000]],\n",
      "\n",
      "         [[204.2195,   0.0000],\n",
      "          [213.5911,   0.0000],\n",
      "          [245.7818,   0.0000],\n",
      "          [ 43.2995,   0.0000],\n",
      "          [ 61.6815,   0.0000],\n",
      "          [258.9698,   0.0000],\n",
      "          [113.8639,   0.0000],\n",
      "          [ 11.6000,   0.0000],\n",
      "          [238.6608,   0.0000],\n",
      "          [225.4152,   0.0000]]]])\n",
      "tensor([[[ 23.4951,  77.8579,  52.8417, 209.0028, 175.5370,   8.5961,  60.1165,\n",
      "          126.7314, 186.9294, 148.6850],\n",
      "         [ 21.0111, 227.6316,  16.5947, 182.8949, 226.3931, 203.6695, 253.8217,\n",
      "          165.8158, 162.2757, 254.9681],\n",
      "         [287.2652, 163.0083,   6.5721,  29.3755, 156.7918, 178.3644, 222.7991,\n",
      "          108.3678, 139.2534, 166.1588],\n",
      "         [ 12.7831, 139.9509, 125.5512,  52.3265, 208.9944,  79.4095,  85.8270,\n",
      "          265.0568,  93.7975, 134.6161],\n",
      "         [127.2725, 121.4351, 204.1686,  65.1949,   7.7407,  82.7763, 271.8460,\n",
      "          160.8336,  25.1880,  30.0603],\n",
      "         [214.0746, 181.2290,  26.0700, 201.6676,  34.5139, 239.8222, 298.1939,\n",
      "          289.8924, 229.2516,  17.5943],\n",
      "         [181.1922, 230.5292, 272.9868, 205.6035, 172.8082,  22.2993, 231.2370,\n",
      "          228.8740,  23.6858, 212.3212],\n",
      "         [ 38.0806,  74.4213, 171.0240, 166.0731, 156.7827, 202.5620, 144.8888,\n",
      "          110.3088,  88.3069,  63.9522],\n",
      "         [265.9257, 251.3316, 139.6694, 284.0551, 236.5329, 220.7447,   4.4483,\n",
      "          289.1667, 268.8907, 107.3503],\n",
      "         [212.5686, 121.9624, 153.0602, 212.3776, 180.1579,  73.0103, 263.3501,\n",
      "          230.0607,  17.2166, 174.2810]],\n",
      "\n",
      "        [[  4.5585, 142.3527,  47.3131,   9.8325, 158.7455, 197.8389, 259.1360,\n",
      "           17.0655,  26.5702, 215.2896],\n",
      "         [189.5171,  53.7480, 126.7499, 265.1505,  70.6316, 131.9029, 188.2923,\n",
      "          187.5067, 105.3509,  44.5508],\n",
      "         [206.9349, 179.3750,   7.8293, 242.3924, 185.6352, 117.3763, 289.1683,\n",
      "          227.9430, 213.1407, 250.3390],\n",
      "         [285.8528, 293.2285, 112.3298,   0.9844,  77.2659, 294.5697, 295.8330,\n",
      "          234.0199,  78.6113, 234.8698],\n",
      "         [256.0861, 221.3190, 245.4673, 297.3487, 195.9584, 141.2700, 236.5265,\n",
      "          166.9491, 200.9186,  88.2679],\n",
      "         [190.9387, 247.4212, 193.0511, 138.0083, 251.4148, 107.5751, 223.8442,\n",
      "          217.9954,  57.5911, 189.2086],\n",
      "         [207.3588,  85.2420, 289.8805, 186.1927,  38.9951, 253.2901, 153.6100,\n",
      "          230.0046, 159.2540,   5.2155],\n",
      "         [ 29.4798, 185.6910, 225.0405,   3.6669, 223.5167, 217.2409,  73.9507,\n",
      "          218.1013, 104.4556,  50.9422],\n",
      "         [264.8647,  86.3658, 114.0745, 249.6844,  51.8479, 207.4627, 119.0959,\n",
      "          276.2213, 279.8814,  26.1208],\n",
      "         [ 24.7802, 198.8270, 184.4317, 108.1788,  49.4818,  88.0829, 180.7710,\n",
      "           67.0135,  94.3279,  95.4379]],\n",
      "\n",
      "        [[  2.3332,  88.5597,  62.8136,  46.0074,  99.4862, 154.6520, 155.0573,\n",
      "          257.2540,  66.2274, 290.9909],\n",
      "         [136.2593,  46.4050,  30.9500, 143.6417, 222.4816, 200.4478, 267.9243,\n",
      "          228.7699,   5.4707,  35.8657],\n",
      "         [121.3029,  55.7284, 264.5975, 231.1380,  64.2339, 114.7014,  91.5010,\n",
      "          239.1461, 240.8268, 279.8974],\n",
      "         [208.6180, 170.6921,  56.5331, 212.9647, 121.6083, 216.6049, 205.2364,\n",
      "          294.6563, 172.0001, 295.5534],\n",
      "         [ 99.1991,  73.4570, 149.7598, 188.4409, 261.3910,  36.9725, 297.0513,\n",
      "          281.0991,   4.1690, 263.1960],\n",
      "         [230.5708, 273.6947,  92.9854, 203.6551, 219.8732, 182.3720, 117.7622,\n",
      "            8.3582, 280.4752, 177.1408],\n",
      "         [ 10.6868,  69.4297, 129.5409,  20.2567, 256.0238, 113.6847, 286.5862,\n",
      "          126.8190,  82.9184, 280.1331],\n",
      "         [226.2163, 102.9874,  32.4145,   1.9337, 150.6180, 250.8619,  74.2006,\n",
      "          245.6798, 184.4855,  84.5555],\n",
      "         [ 83.8827, 243.6698, 263.7338, 243.1683, 164.4037,  28.9788, 236.8713,\n",
      "          165.9432,  15.4653,  73.6467],\n",
      "         [204.2195, 213.5911, 245.7818,  43.2995,  61.6815, 258.9698, 113.8639,\n",
      "           11.6000, 238.6608, 225.4152]]])\n"
     ]
    }
   ],
   "source": [
    "testinp = torch.rand(3,10,10)\n",
    "shape = testinp.shape\n",
    "print(testinp.shape)\n",
    "out = torch.fft.fftn(testinp)\n",
    "print(torch.view_as_real(out).shape)\n",
    "out2 = torch.fft.fftn(out)\n",
    "print(torch.view_as_real(out2).shape)\n",
    "print(out2.real.shape)\n",
    "print(torch.view_as_real(out2))\n",
    "print(out2.real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class scaledDotProduct(nn.Module):\n",
    "    '''\n",
    "        Attention(Q, K, V ) = softmax( QK^T/√d_k)V \n",
    "    \n",
    "    '''\n",
    "    #Takes number of embedded, head_size, context length\n",
    "    def __init__(self, embn, hdim, con_l, drop=0.0):\n",
    "\n",
    "        super(scaledDotProduct, self).__init__()\n",
    "        #dim is (d_k) when sqrt'd it is meant to counter small gradients in large sets of queries and keys\n",
    "        self.k = nn.Linear(embn, hdim, bias=False)\n",
    "        self.q = nn.Linear(embn, hdim, bias=False)\n",
    "        self.v = nn.Linear(embn, hdim, bias=False)\n",
    "        self.d_k = np.sqrt(hdim)\n",
    "\n",
    "        self.register_buffer('mask', torch.tril(torch.ones(con_l,con_l)))\n",
    "        #Simple drop out \n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x, ret_att=False):\n",
    "        #batch X length X dim\n",
    "        B,T,C = x.shape\n",
    "        k = self.k(x)\n",
    "        q = self.q(x)\n",
    "\n",
    "        n = torch.matmul(q, k.transpose(-2,-1)) * k.shape[-1]**-0.5 #BxTxhdim\n",
    "        n = n.masked_fill(self.mask[:T,:T]==0, float('-inf'))\n",
    "        #Drop out referenced later in paper but not in original diagram\n",
    "        att = self.drop(F.softmax(n, dim=-1))\n",
    "\n",
    "        v = self.v(x)\n",
    "\n",
    "        out = torch.matmul(att, v)\n",
    "        if ret_att:\n",
    "            return out, att \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class multiHeadedAttention(nn.Module):\n",
    "    def __init__(self, n_heads, dims, embn, con_l, dropout=0.0):\n",
    "        super(multiHeadedAttention, self).__init__()\n",
    "        #d_k=d_v = dims/h\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.attn = nn.ModuleList([scaledDotProduct(embn, dims, con_l) for _ in range(n_heads)])\n",
    "        #Final linear layer after concat and attention\n",
    "        self.fc = nn.Linear(n_heads*dims, embn)\n",
    "\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.attn], dim=-1)\n",
    "        out = self.drop(self.fc(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fftLayer(nn.Module):\n",
    "    def __init__(self, dims, embn, dropout=0.0):\n",
    "        super(fftLayer, self).__init__()\n",
    "        #d_k=d_v = dims/h\n",
    "\n",
    "        \n",
    "        #Final linear layer after concat and attention\n",
    "        self.fc = nn.Linear(dims, embn)\n",
    "\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.fft.fftn(x)\n",
    "        out = self.drop(self.fc(out.real))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class positionFeedFoward(nn.Module):\n",
    "    def __init__(self, inp, hid, drop=0.0):\n",
    "        super(positionFeedFoward, self).__init__()\n",
    "        self.w1 = nn.Linear(inp,4*hid)\n",
    "        self.w2 = nn.Linear(4*hid,inp)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.w2(F.relu(self.w1(x)))\n",
    "        x = self.drop(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    '''Combinds MultiHeadedAttention and FeeForward, three layers'''\n",
    "    def __init__(self, nheads, embn, con_l, drop=0.0):\n",
    "        super(Decoder, self).__init__()\n",
    "        head_size = embn // nheads\n",
    "        self.slf_attn = multiHeadedAttention(nheads, head_size,embn, con_l, dropout=drop)\n",
    "        \n",
    "        self.ffn = positionFeedFoward(embn, embn, drop=drop)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(embn)\n",
    "        self.norm2 = nn.LayerNorm(embn)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.slf_attn(self.norm1(x))\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fftDecoder(nn.Module):\n",
    "    '''Combinds MultiHeadedAttention and FeeForward, three layers'''\n",
    "    def __init__(self, dims, embn,  drop=0.0):\n",
    "        super(fftDecoder, self).__init__()\n",
    "        \n",
    "        self.slf_fft = fftLayer(dims, embn,  dropout=drop)\n",
    "        \n",
    "        self.ffn = positionFeedFoward(embn, embn, drop=drop)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(embn)\n",
    "        self.norm2 = nn.LayerNorm(embn)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.slf_fft(self.norm1(x))\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pytoch version adapted from here https://pub.aimind.so/creating-sinusoidal-positional-embedding-from-scratch-in-pytorch-98c49e153d6\n",
    "\n",
    "class PosEncoding(nn.Module):\n",
    "    def __init__(self, hid, n_pos=200):\n",
    "        super(PosEncoding, self).__init__()\n",
    "\n",
    "        self.register_buffer('pos_table', self._get_sinusoid_encoding_table(n_pos, hid))\n",
    "\n",
    "    def _get_sinusoid_encoding_table(self, n_pos, hid):\n",
    "\n",
    "        if hid %2 != 0:\n",
    "            raise ValueError(\"Sinusoidal positional embedding cannot apply to odd token embedding dim={}\".format(hid))\n",
    "        \n",
    "        positions = torch.arange(0,n_pos).unsqueeze_(1)\n",
    "        embeds = torch.zeros(n_pos, hid)\n",
    "\n",
    "        denom = torch.pow(10000, 2 * torch.arange(0, hid//2)/2)\n",
    "        embeds[:, 0::2] = torch.sin(positions/denom)\n",
    "        embeds[:, 1::2] = torch.cos(positions/denom)\n",
    "        embeds = embeds.unsqueeze(0)\n",
    "\n",
    "        return embeds\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pos_table[:, :x.size(1)].clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class languageModel(nn.Module):\n",
    "    '''Decoder model'''\n",
    "    def __init__(\n",
    "            self, n_vocab, embn, n_layers, n_head, dropout=0.2 , con_l=200\n",
    "    ):\n",
    "        super(languageModel, self).__init__()\n",
    "        self.con_l = con_l\n",
    "        self.word_emb = nn.Embedding(n_vocab, embn)\n",
    "        self.pos_enc = nn.Embedding(con_l, embn)\n",
    "        attn_stack = [Decoder( n_head, embn, con_l, drop=dropout) for _ in range(n_layers//2)]\n",
    "        fft_stack = [fftDecoder(embn,embn,dropout) for _ in range(n_layers//2)]\n",
    "        self.stack = nn.Sequential(\n",
    "            *[c for items in zip(attn_stack, fft_stack) for c in items]\n",
    "        )\n",
    "       \n",
    "        self.layer_norm = nn.LayerNorm(embn)\n",
    "        self.fc = nn.Linear(embn, n_vocab)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x, tar=None):\n",
    "        #batch, time\n",
    "        B, T = x.shape\n",
    "\n",
    "        tok = self.word_emb(x)\n",
    "        pos = self.pos_enc(torch.arange(T, device=device))\n",
    "        x = tok + pos\n",
    "        x = self.stack(x)\n",
    "        x = self.layer_norm(x)\n",
    "        logits = self.fc(x)\n",
    "\n",
    "        if tar is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            tar = tar.view(B*T)\n",
    "            loss = F.cross_entropy(logits, tar)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, x, max_length):\n",
    "        #x is a BxT array of in current context\n",
    "        for _ in range(max_length):\n",
    "            x_cond = x[:, -self.con_l:]\n",
    "            logits, loss = self(x_cond)\n",
    "            logits = logits[:,-1,:]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            x_next = torch.multinomial(probs, num_samples=1)\n",
    "            x = torch.cat((x, x_next), dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits = model(X)[0]\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = Y.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "step 0: train loss 4.2570, val loss 4.2573\n",
      "step 500: train loss 0.0922, val loss 0.0955\n",
      "step 1000: train loss 0.0654, val loss 0.0678\n",
      "step 1500: train loss 0.0580, val loss 0.0587\n",
      "step 2000: train loss 0.0536, val loss 0.0540\n",
      "step 2500: train loss 0.0507, val loss 0.0514\n",
      "step 3000: train loss 0.0491, val loss 0.0489\n",
      "step 3500: train loss 0.0478, val loss 0.0494\n",
      "step 4000: train loss 0.0468, val loss 0.0483\n",
      "step 4500: train loss 0.0471, val loss 0.0484\n",
      "step 4999: train loss 0.0453, val loss 0.0468\n",
      "\n",
      "o ee ee to  el re lee eeo  a  ee t r ae oe e ee ite  et er os. oe er  iee ia oe  e r t  oeei e e er ian s s elr i oee oe loe ii oe r mr io y e e ee o lo o iy eee er ate ie rdoooieo olr ooe ieee et io lee etee ae iot o leee io o ior eoe oee e eo os ier eee ilr ieo oete red  ooe lte oeet oe eo peea td ea i e ei eeee  oe  io   eee oe e y io ri e ea, e g le ee ieoe eo  ei lr iilt ii oo no oroee s ll  lee et ooo ee let. oe rs a i ee  let tlo it o e erl meer o e iee i eoed lr eee iole  ie r i e et lr \n"
     ]
    }
   ],
   "source": [
    "model = languageModel(vocab_size,  384,12, 12, con_l=50\n",
    "    )\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "#print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "print(next(m.parameters()).is_cuda)\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss(model)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    #B, T, C = logits.shape\n",
    "    #logits = logits.view(B*T, C)\n",
    "    #targets = yb.view(B*T)\n",
    "    #loss = F.cross_entropy(logits, targets)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_length=500)[0].tolist()))\n",
    "#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wherefore art thoueee ee  ilr iaeeee eeee ee ee r ei y oe io oee ee  eo et ie eo  reee  lr d eo la e a  o ea e ea e ae  ae et lr  ialeroieie e tat olor lte r ee ooey d iet dtlr aeo are r lle o oeo ooo e re e ee ta o dro eeele c oe l ooe zeod io oo olee tc uo lo o erree lootooerood ooee oe rrro  aos  eee a le eo ai  oe  leed  oo o a e eremet ri o liee eeee ndd io to ieas  eoer er s iee ir oo dg  ae ee e oe splet iioe a holo ei oiald iola  ird ueortoy eo o d o eoloe eeit  iool et ro ie  ooo ee aet lleeeeeo ee it oe\n"
     ]
    }
   ],
   "source": [
    "sinp = \"wherefore art thou\"\n",
    "sinp = torch.tensor(encode(sinp)).unsqueeze(0).to(device)\n",
    "print(decode(model.generate(sinp, max_length=500)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
